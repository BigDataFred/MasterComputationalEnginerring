return(FPR)
}
###################### 2-Class Problems
# credit (1)
dB1 <- data.frame()
dB1.cM1 <- rbind(c(175,30),c(38,44))# Naive Bayes
dB1.cM2 <- rbind(c(177,28),c(41,41)) # SVM
dB1.cM3 <- rbind(c(190,15),c(59,23)) # kNN (k=10)
dB1.cM4 <- rbind(c(164,41),c(43,39)) # Bagging -J48
dB1.cM5 <- rbind(c(168,37),c(41,41))# Boosting -J48
# kr-vs-kp (2)
dB2 <- data.frame()
dB2.cM1 <- rbind(c(531,47),c(63,464))# Naive Bayes
dB2.cM2 <- rbind(c(546,14),c(54,473)) # SVM
dB2.cM3 <- rbind(c(537,23),c(40,487)) # kNN (k=10)
dB2.cM4 <- rbind(c(541,19),c(23,504)) # Bagging -J48
dB2.cM5 <- rbind(c(532,28),c(12,515))# Boosting -J48
# mushroom (3)
dB3 <- data.frame()
dB3.cM1 <- rbind(c(1400,10),c(120,1232))# Naive Bayes
dB3.cM2 <- rbind(c(1410,0),c(0,1352)) # SVM
dB3.cM3 <- rbind(c(1410,0),c(6,1346)) # kNN (k=10)
dB3.cM4 <- rbind(c(1410,0),c(0,1352)) # Bagging -J48
dB3.cM5 <- rbind(c(1410,0),c(0,1352))# Boosting -J48
# sick (4)
dB4 <- data.frame()
dB4.cM1 <- rbind(c(1191,16),c(12,63))# Naive Bayes
dB4.cM2 <- rbind(c(1193,14),c(17,58)) # SVM
dB4.cM3 <- rbind(c(1198,9),c(29,46)) # kNN (k=10)
dB4.cM4 <- rbind(c(1199,8),c(20,55)) # Bagging -J48
dB4.cM5 <- rbind(c(1198,9),c(20,55))# Boosting -J48
###################### Multi-Class Problems
# hypothyroid (1) c=5
dB5 <- data.frame()
dB5.cM1 <- rbind(c(1171,5,5,0),c(9,57,0,0),c(1,2,30,0),c(2,0,0,0))# Naive Bayes
dB5.cM2 <- rbind(c(1178,1,2,0),c(2,64,0,0),c(1,0,32,0),c(2,0,0,0)) # SVM
dB5.cM3 <- rbind(c(1176,3,3,0),c(42,24,0,0),c(9,3,21,0),c(2,0,0,0)) # kNN (k=10)
dB5.cM4 <- rbind(c(1177,2,2,0),c(3,63,0,0),c(5,0,28,0),c(2,0,0,0)) # Bagging -J48
dB5.cM5 <- rbind(c(1172,7,2,0),c(0,66,0,0),c(1,0,32,0),c(2,0,0,0))# Boosting -J48
# segment (2) c=7
dB6 <- data.frame()
dB6.cM1 <- rbind(c(79,0,0,1,0,0,0),c(0,102,0,0,0,0,0),c(2,0,68,4,15,0,0),c(1,0,0,67,2,5,0),c(3,0,23,3,83,0,1),c(0,0,0,1,0,61,0),c(0,0,0,0,0,0,41))# Naive Bayes
dB6.cM2 <- rbind(c(80,0,0,0,0,0,0),c(0,102,0,0,0,0,0),c(2,0,74,2,11,0,0),c(2,0,1,72,0,0,0),c(0,0,11,4,98,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41)) # SVM
dB6.cM3 <- rbind(c(79,0,0,0,1,0,0),c(0,102,0,0,0,0,0),c(4,0,68,1,15,1,0),c(4,0,1,58,2,10,0),c(0,0,16,2,95,0,0),c(0,0,0,1,0,61,0),c(0,0,0,0,0,0,41)) # kNN (k=10)
dB6.cM4 <- rbind(c(79,0,1,0,0,0,0),c(0,102,0,0,0,0,0),c(1,0,72,0,14,0,2),c(1,0,1,68,5,0,0),c(0,0,7,0,106,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41)) # Bagging -J48
dB6.cM5 <- rbind(c(80,0,0,0,0,0,0),c(0,102,0,0,0,0,0),c(2,0,68,1,14,0,4),c(1,0,1,69,4,0,0),c(0,0,11,0,102,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41))# Boosting -J48
# splice (3) c=3
dB7 <- data.frame()
dB7.cM1 <- rbind(c(258,9,2),c(14,237,8),c(15,12,530))# Naive Bayes
dB7.cM2 <- rbind(c(257,7,5),c(13,240,6),c(5,13,539)) # SVM
dB7.cM3 <- rbind(c(263,4,2),c(25,230,4),c(72,54,431)) # kNN (k=10)
dB7.cM4 <- rbind(c(256,6,7),c(20,231,8),c(16,21,520)) # Bagging -J48
dB7.cM5 <- rbind(c(258,4,7),c(14,237,8),c(13,21,523))# Boosting -J48
# waveform-5000 (4) c= 3
dB8 <- data.frame()
dB8.cM1 <- rbind(c(306,114,131),c(8,544,34),c(3,18,542))# Naive Bayes
dB8.cM2 <- rbind(c(447,52,52),c(55,495,36),c(44,28,491)) # SVM
dB8.cM3 <- rbind(c(404,58,89),c(62,487,37),c(33,38,492)) # kNN (k=10)
dB8.cM4 <- rbind(c(425,57,69),c(56,492,38),c(65,32,466)) # Bagging -J48
dB8.cM5 <- rbind(c(420,68,63),c(78,461,47),c(75,36,452))# Boosting -J48
###################### Accuracy parameters 2-Class
Re <- matrix(nrow=4,ncol=5)
Pr <- matrix(nrow=4,ncol=5)
FPR <- matrix(nrow=4,ncol=5)
for (ix in 1:4){
for (ix2 in 1:5){
outDat <- procCM(eval(parse(text=paste('dB',ix,'.cM',ix2,sep=''))))
Re[ix,ix2]<-calcRe(outDat[1],outDat[3])
Pr[ix,ix2]<-calcPr(outDat[1],outDat[2])
FPR[ix,ix2]<-calcFPR(outDat[1],outDat[2],outDat[4])
}
}
Re1<-apply(Re,2,mean)
Pr1<-apply(Pr,2,mean)
FPR1<-apply(FPR,2,mean)
###################### Accuracy parameters Multi-Class
Re <- matrix(nrow=4,ncol=5)
Pr <- matrix(nrow=4,ncol=5)
FPR <- matrix(nrow=4,ncol=5)
for (ix in 5:8){
for (ix2 in 1:5){
outDat <- procCM(eval(parse(text=paste('dB',ix,'.cM',ix2,sep=''))))
Re[ix-4,ix2]<-calcRe(outDat[1],outDat[3])
Pr[ix-4,ix2]<-calcPr(outDat[1],outDat[2])
FPR[ix-4,ix2]<-calcFPR(outDat[1],outDat[2],outDat[4])
}
}
Re2<-apply(Re,2,mean)
Pr2<-apply(Pr,2,mean)
FPR2<-apply(FPR,2,mean)
###################### figures
C = c('blue','green','yellow','orange','red')
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='Precision',ylab='Recall')
for (ix in 1:length(Re1)){
lines(c(0,Pr1[ix],1),c(1,Re1[ix],0),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='Precision',ylab='Recall')
for (ix in 1:length(Re1)){
lines(c(0,Pr2[ix],1),c(1,Re2[ix],0),type='b',col='red')
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='False positive rate',ylab='True positive rate')
for (ix in 1:length(Pr1)){
lines(c(0,FPR1[ix],1),c(0,Pr1[ix],1),type='b',col='red')
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='False positive rate',ylab='True positive rate')
for (ix in 1:length(Pr2)){
lines(c(0,FPR2[ix],1),c(0,Pr2[ix],1),type='b',col='red')
}
library(foreign)
library(farff)
graphics.off()
setwd("/Users/froux/MasterIngeneriaComputacional2018/EAD-MD-v/UCI/")
getwd()
dbList <- list.files()
fsz <- vector(length=length(dbList))
ix <-0
for (fileName in dbList){
print(fileName)
ix<- ix+1
fsz[ix]<-file.size(fileName)/1000000
}
#dbList <- dbList[order(fsz)]
cbind(dbList,fsz[order(fsz)])
nC <- vector(length=6)
nA <- vector(length=6)
nI <- vector(length=6)
cnt<-0
for (ix in 1:length(fsz) ){ # seq.int(length(fsz)-6,length(fsz))
cnt <- cnt+1
print(c('Loading:',dbList[ix]))
tmp <- read.arff(dbList[ix])
nC[cnt] <-length(unique(tmp[,ncol(tmp)]))
nA[cnt] <-ncol(tmp)-1
nI[cnt] <-nrow(tmp)
}
selIx1 <- intersect(which(nC==2),which(nI>=1000))
selIx2 <- intersect(which(nC>=3 & nC <=7),which(nI>=1000))
par( mfrow=c(2,2) )
plot(fsz,type='l',col='grey',ylab='Database size [Mb]',xlab='Database index')
lines(selIx1,fsz[selIx1],type='p',col='red')
lines(selIx2,fsz[selIx2],type='p',col='blue')
plot(nC,type='l',col='grey',ylab='Number of classes',xlab='Database index')
lines(selIx1,nC[selIx1],type='p',col='grey')
lines(selIx2,nC[selIx2],type='p',col='blue')
plot(fsz,nA,type='p',col='grey',ylab='Number of attributes',xlab='Database size [Mb]')
lines(fsz[selIx1],nA[selIx1],type='p',col='red')
lines(fsz[selIx2],nA[selIx2],type='p',col='blue')
plot(fsz,nI,type='p',col='grey',ylab='Number of instances',xlab='Database size [Mb]')
lines(fsz[selIx1],nI[selIx1],type='p',col='red')
lines(fsz[selIx2],nI[selIx2],type='p',col='blue')
dbList[selIx1]
dbList[selIx2]
procCM <- function(cM){
if ( ncol(cM)>2 & nrow(cM)>2 ){
d<-diag(cM)
TP <- vector(length=length(d))
TN <- vector(length=length(d))
FP <- vector(length=length(d))
FN <- vector(length=length(d))
for (ix in 1:length(d)){
TP[ix]<- d[ix]
FP[ix] <- sum(cM[,ix])-d[ix]
FN[ix] <- sum(cM[ix,])-d[ix]
TN[ix]<- sum(cM)-(TP[ix]+FP[ix]+FN[ix])
}
avgTP <-0
avgFP <-0
avgFN <-0
avgTN <-0
for (ix in 1:length(TP)){
avgTP = avgTP+sum(cM[ix,])/sum(cM)*TP[ix]
avgFP = avgFP+sum(cM[ix,])/sum(cM)*FP[ix]
avgFN = avgFN+sum(cM[ix,])/sum(cM)*FN[ix]
avgTN = avgTN+sum(cM[ix,])/sum(cM)*TN[ix]
}
TP <- avgTP
FP <- avgFP
FN <- avgFN
TN <- avgTN
} else {
TP <- cM[1,1]
FP <- cM[2,1]
FN <- cM[1,2]
TN <- cM[2,2]
}
outDat <- vector(length=4)
outDat[1]<-TP
outDat[2]<-FP
outDat[3]<-FN
outDat[4]<-TN
return(outDat)
}
calcRe <- function(TP,FN){
Re <- TP/(TP+FN)
return(Re)
}
calcPr <- function(TP,FP){
Pr <- TP/(TP+FP)
return(Pr)
}
calcFPR <- function(TP,FP,TN){
FPR <- FP/(TP+TN)
return(FPR)
}
###################### 2-Class Problems
# credit (1)
dB1 <- data.frame()
dB1.cM1 <- rbind(c(175,30),c(38,44))# Naive Bayes
dB1.cM2 <- rbind(c(177,28),c(41,41)) # SVM
dB1.cM3 <- rbind(c(190,15),c(59,23)) # kNN (k=10)
dB1.cM4 <- rbind(c(164,41),c(43,39)) # Bagging -J48
dB1.cM5 <- rbind(c(168,37),c(41,41))# Boosting -J48
# kr-vs-kp (2)
dB2 <- data.frame()
dB2.cM1 <- rbind(c(531,47),c(63,464))# Naive Bayes
dB2.cM2 <- rbind(c(546,14),c(54,473)) # SVM
dB2.cM3 <- rbind(c(537,23),c(40,487)) # kNN (k=10)
dB2.cM4 <- rbind(c(541,19),c(23,504)) # Bagging -J48
dB2.cM5 <- rbind(c(532,28),c(12,515))# Boosting -J48
# mushroom (3)
dB3 <- data.frame()
dB3.cM1 <- rbind(c(1400,10),c(120,1232))# Naive Bayes
dB3.cM2 <- rbind(c(1410,0),c(0,1352)) # SVM
dB3.cM3 <- rbind(c(1410,0),c(6,1346)) # kNN (k=10)
dB3.cM4 <- rbind(c(1410,0),c(0,1352)) # Bagging -J48
dB3.cM5 <- rbind(c(1410,0),c(0,1352))# Boosting -J48
# sick (4)
dB4 <- data.frame()
dB4.cM1 <- rbind(c(1191,16),c(12,63))# Naive Bayes
dB4.cM2 <- rbind(c(1193,14),c(17,58)) # SVM
dB4.cM3 <- rbind(c(1198,9),c(29,46)) # kNN (k=10)
dB4.cM4 <- rbind(c(1199,8),c(20,55)) # Bagging -J48
dB4.cM5 <- rbind(c(1198,9),c(20,55))# Boosting -J48
###################### Multi-Class Problems
# hypothyroid (1) c=5
dB5 <- data.frame()
dB5.cM1 <- rbind(c(1171,5,5,0),c(9,57,0,0),c(1,2,30,0),c(2,0,0,0))# Naive Bayes
dB5.cM2 <- rbind(c(1178,1,2,0),c(2,64,0,0),c(1,0,32,0),c(2,0,0,0)) # SVM
dB5.cM3 <- rbind(c(1176,3,3,0),c(42,24,0,0),c(9,3,21,0),c(2,0,0,0)) # kNN (k=10)
dB5.cM4 <- rbind(c(1177,2,2,0),c(3,63,0,0),c(5,0,28,0),c(2,0,0,0)) # Bagging -J48
dB5.cM5 <- rbind(c(1172,7,2,0),c(0,66,0,0),c(1,0,32,0),c(2,0,0,0))# Boosting -J48
# segment (2) c=7
dB6 <- data.frame()
dB6.cM1 <- rbind(c(79,0,0,1,0,0,0),c(0,102,0,0,0,0,0),c(2,0,68,4,15,0,0),c(1,0,0,67,2,5,0),c(3,0,23,3,83,0,1),c(0,0,0,1,0,61,0),c(0,0,0,0,0,0,41))# Naive Bayes
dB6.cM2 <- rbind(c(80,0,0,0,0,0,0),c(0,102,0,0,0,0,0),c(2,0,74,2,11,0,0),c(2,0,1,72,0,0,0),c(0,0,11,4,98,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41)) # SVM
dB6.cM3 <- rbind(c(79,0,0,0,1,0,0),c(0,102,0,0,0,0,0),c(4,0,68,1,15,1,0),c(4,0,1,58,2,10,0),c(0,0,16,2,95,0,0),c(0,0,0,1,0,61,0),c(0,0,0,0,0,0,41)) # kNN (k=10)
dB6.cM4 <- rbind(c(79,0,1,0,0,0,0),c(0,102,0,0,0,0,0),c(1,0,72,0,14,0,2),c(1,0,1,68,5,0,0),c(0,0,7,0,106,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41)) # Bagging -J48
dB6.cM5 <- rbind(c(80,0,0,0,0,0,0),c(0,102,0,0,0,0,0),c(2,0,68,1,14,0,4),c(1,0,1,69,4,0,0),c(0,0,11,0,102,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41))# Boosting -J48
# splice (3) c=3
dB7 <- data.frame()
dB7.cM1 <- rbind(c(258,9,2),c(14,237,8),c(15,12,530))# Naive Bayes
dB7.cM2 <- rbind(c(257,7,5),c(13,240,6),c(5,13,539)) # SVM
dB7.cM3 <- rbind(c(263,4,2),c(25,230,4),c(72,54,431)) # kNN (k=10)
dB7.cM4 <- rbind(c(256,6,7),c(20,231,8),c(16,21,520)) # Bagging -J48
dB7.cM5 <- rbind(c(258,4,7),c(14,237,8),c(13,21,523))# Boosting -J48
# waveform-5000 (4) c= 3
dB8 <- data.frame()
dB8.cM1 <- rbind(c(306,114,131),c(8,544,34),c(3,18,542))# Naive Bayes
dB8.cM2 <- rbind(c(447,52,52),c(55,495,36),c(44,28,491)) # SVM
dB8.cM3 <- rbind(c(404,58,89),c(62,487,37),c(33,38,492)) # kNN (k=10)
dB8.cM4 <- rbind(c(425,57,69),c(56,492,38),c(65,32,466)) # Bagging -J48
dB8.cM5 <- rbind(c(420,68,63),c(78,461,47),c(75,36,452))# Boosting -J48
###################### Accuracy parameters 2-Class
Re <- matrix(nrow=4,ncol=5)
Pr <- matrix(nrow=4,ncol=5)
FPR <- matrix(nrow=4,ncol=5)
for (ix in 1:4){
for (ix2 in 1:5){
outDat <- procCM(eval(parse(text=paste('dB',ix,'.cM',ix2,sep=''))))
Re[ix,ix2]<-calcRe(outDat[1],outDat[3])
Pr[ix,ix2]<-calcPr(outDat[1],outDat[2])
FPR[ix,ix2]<-calcFPR(outDat[1],outDat[2],outDat[4])
}
}
Re1<-apply(Re,2,mean)
Pr1<-apply(Pr,2,mean)
FPR1<-apply(FPR,2,mean)
###################### Accuracy parameters Multi-Class
Re <- matrix(nrow=4,ncol=5)
Pr <- matrix(nrow=4,ncol=5)
FPR <- matrix(nrow=4,ncol=5)
for (ix in 5:8){
for (ix2 in 1:5){
outDat <- procCM(eval(parse(text=paste('dB',ix,'.cM',ix2,sep=''))))
Re[ix-4,ix2]<-calcRe(outDat[1],outDat[3])
Pr[ix-4,ix2]<-calcPr(outDat[1],outDat[2])
FPR[ix-4,ix2]<-calcFPR(outDat[1],outDat[2],outDat[4])
}
}
Re2<-apply(Re,2,mean)
Pr2<-apply(Pr,2,mean)
FPR2<-apply(FPR,2,mean)
###################### figures
C = c('blue','green','yellow','orange','red')
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='Precision',ylab='Recall')
for (ix in 1:length(Re1)){
lines(c(0,Pr1[ix],1),c(1,Re1[ix],0),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='Precision',ylab='Recall')
for (ix in 1:length(Re1)){
lines(c(0,Pr2[ix],1),c(1,Re2[ix],0),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='False positive rate',ylab='True positive rate')
for (ix in 1:length(Pr1)){
lines(c(0,FPR1[ix],1),c(0,Pr1[ix],1),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='False positive rate',ylab='True positive rate')
for (ix in 1:length(Pr2)){
lines(c(0,FPR2[ix],1),c(0,Pr2[ix],1),type='b',col=C[ix])
}
library(foreign)
library(farff)
graphics.off()
setwd("/Users/froux/MasterIngeneriaComputacional2018/EAD-MD-v/UCI/")
getwd()
dbList <- list.files()
fsz <- vector(length=length(dbList))
ix <-0
for (fileName in dbList){
print(fileName)
ix<- ix+1
fsz[ix]<-file.size(fileName)/1000000
}
#dbList <- dbList[order(fsz)]
cbind(dbList,fsz[order(fsz)])
nC <- vector(length=6)
nA <- vector(length=6)
nI <- vector(length=6)
cnt<-0
for (ix in 1:length(fsz) ){ # seq.int(length(fsz)-6,length(fsz))
cnt <- cnt+1
print(c('Loading:',dbList[ix]))
tmp <- read.arff(dbList[ix])
nC[cnt] <-length(unique(tmp[,ncol(tmp)]))
nA[cnt] <-ncol(tmp)-1
nI[cnt] <-nrow(tmp)
}
selIx1 <- intersect(which(nC==2),which(nI>=1000))
selIx2 <- intersect(which(nC>=3 & nC <=7),which(nI>=1000))
par( mfrow=c(2,2) )
plot(fsz,type='l',col='grey',ylab='Database size [Mb]',xlab='Database index')
lines(selIx1,fsz[selIx1],type='p',col='red')
lines(selIx2,fsz[selIx2],type='p',col='blue')
plot(nC,type='l',col='grey',ylab='Number of classes',xlab='Database index')
lines(selIx1,nC[selIx1],type='p',col='grey')
lines(selIx2,nC[selIx2],type='p',col='blue')
plot(fsz,nA,type='p',col='grey',ylab='Number of attributes',xlab='Database size [Mb]')
lines(fsz[selIx1],nA[selIx1],type='p',col='red')
lines(fsz[selIx2],nA[selIx2],type='p',col='blue')
plot(fsz,nI,type='p',col='grey',ylab='Number of instances',xlab='Database size [Mb]')
lines(fsz[selIx1],nI[selIx1],type='p',col='red')
lines(fsz[selIx2],nI[selIx2],type='p',col='blue')
dbList[selIx1]
dbList[selIx2]
procCM <- function(cM){
if ( ncol(cM)>2 & nrow(cM)>2 ){
d<-diag(cM)
TP <- vector(length=length(d))
TN <- vector(length=length(d))
FP <- vector(length=length(d))
FN <- vector(length=length(d))
for (ix in 1:length(d)){
TP[ix]<- d[ix]
FP[ix] <- sum(cM[,ix])-d[ix]
FN[ix] <- sum(cM[ix,])-d[ix]
TN[ix]<- sum(cM)-(TP[ix]+FP[ix]+FN[ix])
}
avgTP <-0
avgFP <-0
avgFN <-0
avgTN <-0
for (ix in 1:length(TP)){
avgTP = avgTP+sum(cM[ix,])/sum(cM)*TP[ix]
avgFP = avgFP+sum(cM[ix,])/sum(cM)*FP[ix]
avgFN = avgFN+sum(cM[ix,])/sum(cM)*FN[ix]
avgTN = avgTN+sum(cM[ix,])/sum(cM)*TN[ix]
}
TP <- avgTP
FP <- avgFP
FN <- avgFN
TN <- avgTN
} else {
TP <- cM[1,1]
FP <- cM[2,1]
FN <- cM[1,2]
TN <- cM[2,2]
}
outDat <- vector(length=4)
outDat[1]<-TP
outDat[2]<-FP
outDat[3]<-FN
outDat[4]<-TN
return(outDat)
}
calcRe <- function(TP,FN){
Re <- TP/(TP+FN)
return(Re)
}
calcPr <- function(TP,FP){
Pr <- TP/(TP+FP)
return(Pr)
}
calcFPR <- function(TP,FP,TN){
FPR <- FP/(TP+TN)
return(FPR)
}
###################### 2-Class Problems
# credit (1)
dB1 <- data.frame()
dB1.cM1 <- rbind(c(175,30),c(38,44))# Naive Bayes
dB1.cM2 <- rbind(c(177,28),c(41,41)) # SVM
dB1.cM3 <- rbind(c(190,15),c(59,23)) # kNN (k=10)
dB1.cM4 <- rbind(c(164,41),c(43,39)) # Bagging -J48
dB1.cM5 <- rbind(c(168,37),c(41,41))# Boosting -J48
# kr-vs-kp (2)
dB2 <- data.frame()
dB2.cM1 <- rbind(c(531,47),c(63,464))# Naive Bayes
dB2.cM2 <- rbind(c(546,14),c(54,473)) # SVM
dB2.cM3 <- rbind(c(537,23),c(40,487)) # kNN (k=10)
dB2.cM4 <- rbind(c(541,19),c(23,504)) # Bagging -J48
dB2.cM5 <- rbind(c(532,28),c(12,515))# Boosting -J48
# mushroom (3)
dB3 <- data.frame()
dB3.cM1 <- rbind(c(1400,10),c(120,1232))# Naive Bayes
dB3.cM2 <- rbind(c(1410,0),c(0,1352)) # SVM
dB3.cM3 <- rbind(c(1410,0),c(6,1346)) # kNN (k=10)
dB3.cM4 <- rbind(c(1410,0),c(0,1352)) # Bagging -J48
dB3.cM5 <- rbind(c(1410,0),c(0,1352))# Boosting -J48
# sick (4)
dB4 <- data.frame()
dB4.cM1 <- rbind(c(1191,16),c(12,63))# Naive Bayes
dB4.cM2 <- rbind(c(1193,14),c(17,58)) # SVM
dB4.cM3 <- rbind(c(1198,9),c(29,46)) # kNN (k=10)
dB4.cM4 <- rbind(c(1199,8),c(20,55)) # Bagging -J48
dB4.cM5 <- rbind(c(1198,9),c(20,55))# Boosting -J48
###################### Multi-Class Problems
# hypothyroid (1) c=5
dB5 <- data.frame()
dB5.cM1 <- rbind(c(1171,5,5,0),c(9,57,0,0),c(1,2,30,0),c(2,0,0,0))# Naive Bayes
dB5.cM2 <- rbind(c(1178,1,2,0),c(2,64,0,0),c(1,0,32,0),c(2,0,0,0)) # SVM
dB5.cM3 <- rbind(c(1176,3,3,0),c(42,24,0,0),c(9,3,21,0),c(2,0,0,0)) # kNN (k=10)
dB5.cM4 <- rbind(c(1177,2,2,0),c(3,63,0,0),c(5,0,28,0),c(2,0,0,0)) # Bagging -J48
dB5.cM5 <- rbind(c(1172,7,2,0),c(0,66,0,0),c(1,0,32,0),c(2,0,0,0))# Boosting -J48
# segment (2) c=7
dB6 <- data.frame()
dB6.cM1 <- rbind(c(79,0,0,1,0,0,0),c(0,102,0,0,0,0,0),c(2,0,68,4,15,0,0),c(1,0,0,67,2,5,0),c(3,0,23,3,83,0,1),c(0,0,0,1,0,61,0),c(0,0,0,0,0,0,41))# Naive Bayes
dB6.cM2 <- rbind(c(80,0,0,0,0,0,0),c(0,102,0,0,0,0,0),c(2,0,74,2,11,0,0),c(2,0,1,72,0,0,0),c(0,0,11,4,98,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41)) # SVM
dB6.cM3 <- rbind(c(79,0,0,0,1,0,0),c(0,102,0,0,0,0,0),c(4,0,68,1,15,1,0),c(4,0,1,58,2,10,0),c(0,0,16,2,95,0,0),c(0,0,0,1,0,61,0),c(0,0,0,0,0,0,41)) # kNN (k=10)
dB6.cM4 <- rbind(c(79,0,1,0,0,0,0),c(0,102,0,0,0,0,0),c(1,0,72,0,14,0,2),c(1,0,1,68,5,0,0),c(0,0,7,0,106,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41)) # Bagging -J48
dB6.cM5 <- rbind(c(80,0,0,0,0,0,0),c(0,102,0,0,0,0,0),c(2,0,68,1,14,0,4),c(1,0,1,69,4,0,0),c(0,0,11,0,102,0,0),c(0,0,0,0,0,62,0),c(0,0,0,0,0,0,41))# Boosting -J48
# splice (3) c=3
dB7 <- data.frame()
dB7.cM1 <- rbind(c(258,9,2),c(14,237,8),c(15,12,530))# Naive Bayes
dB7.cM2 <- rbind(c(257,7,5),c(13,240,6),c(5,13,539)) # SVM
dB7.cM3 <- rbind(c(263,4,2),c(25,230,4),c(72,54,431)) # kNN (k=10)
dB7.cM4 <- rbind(c(256,6,7),c(20,231,8),c(16,21,520)) # Bagging -J48
dB7.cM5 <- rbind(c(258,4,7),c(14,237,8),c(13,21,523))# Boosting -J48
# waveform-5000 (4) c= 3
dB8 <- data.frame()
dB8.cM1 <- rbind(c(306,114,131),c(8,544,34),c(3,18,542))# Naive Bayes
dB8.cM2 <- rbind(c(447,52,52),c(55,495,36),c(44,28,491)) # SVM
dB8.cM3 <- rbind(c(404,58,89),c(62,487,37),c(33,38,492)) # kNN (k=10)
dB8.cM4 <- rbind(c(425,57,69),c(56,492,38),c(65,32,466)) # Bagging -J48
dB8.cM5 <- rbind(c(420,68,63),c(78,461,47),c(75,36,452))# Boosting -J48
###################### Accuracy parameters 2-Class
Re <- matrix(nrow=4,ncol=5)
Pr <- matrix(nrow=4,ncol=5)
FPR <- matrix(nrow=4,ncol=5)
for (ix in 1:4){
for (ix2 in 1:5){
outDat <- procCM(eval(parse(text=paste('dB',ix,'.cM',ix2,sep=''))))
Re[ix,ix2]<-calcRe(outDat[1],outDat[3])
Pr[ix,ix2]<-calcPr(outDat[1],outDat[2])
FPR[ix,ix2]<-calcFPR(outDat[1],outDat[2],outDat[4])
}
}
Re1<-apply(Re,2,mean)
Pr1<-apply(Pr,2,mean)
FPR1<-apply(FPR,2,mean)
###################### Accuracy parameters Multi-Class
Re <- matrix(nrow=4,ncol=5)
Pr <- matrix(nrow=4,ncol=5)
FPR <- matrix(nrow=4,ncol=5)
for (ix in 5:8){
for (ix2 in 1:5){
outDat <- procCM(eval(parse(text=paste('dB',ix,'.cM',ix2,sep=''))))
Re[ix-4,ix2]<-calcRe(outDat[1],outDat[3])
Pr[ix-4,ix2]<-calcPr(outDat[1],outDat[2])
FPR[ix-4,ix2]<-calcFPR(outDat[1],outDat[2],outDat[4])
}
}
Re2<-apply(Re,2,mean)
Pr2<-apply(Pr,2,mean)
FPR2<-apply(FPR,2,mean)
###################### figures
C = c('blue','green','yellow','orange','red')
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='Precision',ylab='Recall',main='2-classe problems')
for (ix in 1:length(Re1)){
lines(c(0,Pr1[ix],1),c(1,Re1[ix],0),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='Precision',ylab='Recall',main='Multi-classe problems')
for (ix in 1:length(Re1)){
lines(c(0,Pr2[ix],1),c(1,Re2[ix],0),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='False positive rate',ylab='True positive rate',main='2-classe problems')
for (ix in 1:length(Pr1)){
lines(c(0,FPR1[ix],1),c(0,Pr1[ix],1),type='b',col=C[ix])
}
plot(seq.int(0,1,by=0.1),seq.int(0,1,by=0.1),type='l',col='black',xlab='False positive rate',ylab='True positive rate',main='Multi-classe problems')
for (ix in 1:length(Pr2)){
lines(c(0,FPR2[ix],1),c(0,Pr2[ix],1),type='b',col=C[ix])
}
